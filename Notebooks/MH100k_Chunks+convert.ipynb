{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from azureml.core import Dataset\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "import os\n",
        "from azureml.core import Workspace, Datastore, Run, Model, Experiment\n",
        "import gc\n",
        "import pickle"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1721301683030
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ML client\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "\n",
        "#WS Init -> for persistance\n",
        "ws = Workspace.from_config()  # Assumes there is a config.json file in the current directory\n",
        "datastore = ws.get_default_datastore()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1721301728479
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load a small sample to determine the number of columns\n",
        "data_assetTest = ml_client.data.get(\"MH100k_1\", version=\"1\")\n",
        "sample_data_test = pd.read_csv(data_assetTest.path, nrows=5)  # Load the first 5 rows\n",
        "#experiment = Experiment(workspace=ws, name='malware-detection-experiment3')\n",
        "#run = experiment.start_logging()\n",
        "sample_data_test"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "                                              SHA256            NOME  \\\n0  080da3f89e42250d7462e17b40535cfca9b1a6a8370a31...  2019 شاب دوزي‎   \n1  461760796dd7789673cfaf68383da103033d54eb4a5267...           Ishas   \n2  dab8b14c3178b15200b23e47cecb9cc26b51c87d599ac0...       Lashes&Go   \n3  db802025f9ec474d79793ac2aac556d2b52162ebc493e2...            58到家   \n4  a44920abdd4915117412ad7695b8d95a1da5edfa513b09...      TEDDY AFRO   \n\n                              PACOTE  API_MIN  API  Permission::WAKE_LOCK  \\\n0          com.arabprod.aghani.douzi       10   26                      1   \n1  appinventor.ai_shameertanur.Ishas        7   28                      0   \n2              net.flowww.z.sk596381       16   26                      1   \n3                  com.wuba.jiazheng       17   25                      1   \n4       million.t.com.teddy.afro.com       16   27                      0   \n\n   Permission::WRITE_EXTERNAL_STORAGE  Permission::ACCESS_NETWORK_STATE  \\\n0                                   1                                 1   \n1                                   0                                 1   \n2                                   1                                 1   \n3                                   1                                 1   \n4                                   1                                 1   \n\n   Permission::WRITE_SETTINGS  Permission::INTERNET  ...  \\\n0                           1                     1  ...   \n1                           0                     1  ...   \n2                           0                     1  ...   \n3                           1                     1  ...   \n4                           0                     1  ...   \n\n   APICall::Landroid/widget/ListView.setScaleX()  \\\n0                                              0   \n1                                              0   \n2                                              0   \n3                                              0   \n4                                              0   \n\n   APICall::Landroid/widget/ListView.getScaleX()  \\\n0                                              0   \n1                                              0   \n2                                              0   \n3                                              0   \n4                                              0   \n\n   APICall::Landroid/widget/ScrollView.setOnApplyWindowInsetsListener()  \\\n0                                                  0                      \n1                                                  0                      \n2                                                  0                      \n3                                                  0                      \n4                                                  0                      \n\n   APICall::Landroid/widget/ExpandableListView.setNextFocusUpId()  \\\n0                                                  0                \n1                                                  0                \n2                                                  0                \n3                                                  0                \n4                                                  0                \n\n   APICall::Landroid/widget/ScrollView.setSoundEffectsEnabled()  \\\n0                                                  0              \n1                                                  0              \n2                                                  0              \n3                                                  0              \n4                                                  0              \n\n   APICall::Landroid/widget/TableRow.getLeft()  \\\n0                                            0   \n1                                            0   \n2                                            0   \n3                                            0   \n4                                            0   \n\n   APICall::Landroid/widget/HorizontalScrollView.onKeyUp()  \\\n0                                                  0         \n1                                                  0         \n2                                                  0         \n3                                                  0         \n4                                                  0         \n\n   APICall::Landroid/widget/RatingBar.isFocusable()  \\\n0                                                 0   \n1                                                 0   \n2                                                 0   \n3                                                 0   \n4                                                 0   \n\n   APICall::Landroid/preference/ListPreference.getView()  \\\n0                                                  0       \n1                                                  0       \n2                                                  0       \n3                                                  0       \n4                                                  0       \n\n   APICall::Landroid/widget/LinearLayout.computeHorizontalScrollExtent()  \n0                                                  0                      \n1                                                  0                      \n2                                                  0                      \n3                                                  0                      \n4                                                  0                      \n\n[5 rows x 24842 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SHA256</th>\n      <th>NOME</th>\n      <th>PACOTE</th>\n      <th>API_MIN</th>\n      <th>API</th>\n      <th>Permission::WAKE_LOCK</th>\n      <th>Permission::WRITE_EXTERNAL_STORAGE</th>\n      <th>Permission::ACCESS_NETWORK_STATE</th>\n      <th>Permission::WRITE_SETTINGS</th>\n      <th>Permission::INTERNET</th>\n      <th>...</th>\n      <th>APICall::Landroid/widget/ListView.setScaleX()</th>\n      <th>APICall::Landroid/widget/ListView.getScaleX()</th>\n      <th>APICall::Landroid/widget/ScrollView.setOnApplyWindowInsetsListener()</th>\n      <th>APICall::Landroid/widget/ExpandableListView.setNextFocusUpId()</th>\n      <th>APICall::Landroid/widget/ScrollView.setSoundEffectsEnabled()</th>\n      <th>APICall::Landroid/widget/TableRow.getLeft()</th>\n      <th>APICall::Landroid/widget/HorizontalScrollView.onKeyUp()</th>\n      <th>APICall::Landroid/widget/RatingBar.isFocusable()</th>\n      <th>APICall::Landroid/preference/ListPreference.getView()</th>\n      <th>APICall::Landroid/widget/LinearLayout.computeHorizontalScrollExtent()</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>080da3f89e42250d7462e17b40535cfca9b1a6a8370a31...</td>\n      <td>2019 شاب دوزي‎</td>\n      <td>com.arabprod.aghani.douzi</td>\n      <td>10</td>\n      <td>26</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>461760796dd7789673cfaf68383da103033d54eb4a5267...</td>\n      <td>Ishas</td>\n      <td>appinventor.ai_shameertanur.Ishas</td>\n      <td>7</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dab8b14c3178b15200b23e47cecb9cc26b51c87d599ac0...</td>\n      <td>Lashes&amp;Go</td>\n      <td>net.flowww.z.sk596381</td>\n      <td>16</td>\n      <td>26</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>db802025f9ec474d79793ac2aac556d2b52162ebc493e2...</td>\n      <td>58到家</td>\n      <td>com.wuba.jiazheng</td>\n      <td>17</td>\n      <td>25</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a44920abdd4915117412ad7695b8d95a1da5edfa513b09...</td>\n      <td>TEDDY AFRO</td>\n      <td>million.t.com.teddy.afro.com</td>\n      <td>16</td>\n      <td>27</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24842 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1721301856942
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.complete()"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721214753739
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "Run(Experiment: malware-detection-experiment3,\nId: 0f19fdec-6393-40e9-80ad-332a1a256d7b,\nType: None,\nStatus: Running)",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>malware-detection-experiment3</td><td>0f19fdec-6393-40e9-80ad-332a1a256d7b</td><td></td><td>Running</td><td><a href=\"https://ml.azure.com/runs/0f19fdec-6393-40e9-80ad-332a1a256d7b?wsid=/subscriptions/439ab49f-8192-408d-b924-846fbe36dfc7/resourcegroups/stefanshkenderov/workspaces/stefan&amp;tid=9ad49f3b-bbca-4baf-9e1d-bfeae167d01a\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.Run?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721214754042
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_asset = ml_client.data.get(\"MH100k_2\", version=\"1\")\n"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721216678108
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load a small sample to determine input size\n",
        "sample_data = pd.read_csv(data_asset.path, nrows=10)  # Load the first 10 rows\n",
        "non_numerical_columns = sample_data.select_dtypes(exclude=[int, float]).columns.tolist()\n",
        "sample_data=sample_data.drop(columns=non_numerical_columns)\n",
        "\n",
        "input_size = sample_data.shape[1] - 1  # Subtract 1 for the 'CLASS' column\n",
        "\n",
        "display(sample_data)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "   API_MIN  API  Permission::WAKE_LOCK  Permission::WRITE_EXTERNAL_STORAGE  \\\n0       10   26                      1                                   1   \n1        7   28                      0                                   0   \n2       16   26                      1                                   1   \n3       17   25                      1                                   1   \n4       16   27                      0                                   1   \n5       21   29                      1                                   1   \n6       21   29                      1                                   1   \n7       15   23                      1                                   0   \n8       16   21                      0                                   1   \n9       21   29                      0                                   0   \n\n   Permission::ACCESS_NETWORK_STATE  Permission::WRITE_SETTINGS  \\\n0                                 1                           1   \n1                                 1                           0   \n2                                 1                           0   \n3                                 1                           1   \n4                                 1                           0   \n5                                 1                           0   \n6                                 1                           1   \n7                                 1                           0   \n8                                 1                           0   \n9                                 1                           0   \n\n   Permission::INTERNET  Intent::AUDIO_BECOMING_NOISY  \\\n0                     1                             1   \n1                     1                             0   \n2                     1                             0   \n3                     1                             0   \n4                     1                             0   \n5                     1                             0   \n6                     1                             0   \n7                     1                             0   \n8                     1                             0   \n9                     1                             0   \n\n   APICall::Landroid/content/Intent.toUri()  \\\n0                                         1   \n1                                         0   \n2                                         1   \n3                                         0   \n4                                         1   \n5                                         0   \n6                                         0   \n7                                         1   \n8                                         0   \n9                                         1   \n\n   APICall::Landroid/view/View.setTag()  ...  \\\n0                                     1  ...   \n1                                     1  ...   \n2                                     1  ...   \n3                                     0  ...   \n4                                     1  ...   \n5                                     1  ...   \n6                                     0  ...   \n7                                     1  ...   \n8                                     1  ...   \n9                                     1  ...   \n\n   APICall::Landroid/widget/ListView.setScaleX()  \\\n0                                              0   \n1                                              0   \n2                                              0   \n3                                              0   \n4                                              0   \n5                                              0   \n6                                              0   \n7                                              0   \n8                                              0   \n9                                              0   \n\n   APICall::Landroid/widget/ListView.getScaleX()  \\\n0                                              0   \n1                                              0   \n2                                              0   \n3                                              0   \n4                                              0   \n5                                              0   \n6                                              0   \n7                                              0   \n8                                              0   \n9                                              0   \n\n   APICall::Landroid/widget/ScrollView.setOnApplyWindowInsetsListener()  \\\n0                                                  0                      \n1                                                  0                      \n2                                                  0                      \n3                                                  0                      \n4                                                  0                      \n5                                                  0                      \n6                                                  0                      \n7                                                  0                      \n8                                                  0                      \n9                                                  0                      \n\n   APICall::Landroid/widget/ExpandableListView.setNextFocusUpId()  \\\n0                                                  0                \n1                                                  0                \n2                                                  0                \n3                                                  0                \n4                                                  0                \n5                                                  0                \n6                                                  0                \n7                                                  0                \n8                                                  0                \n9                                                  0                \n\n   APICall::Landroid/widget/ScrollView.setSoundEffectsEnabled()  \\\n0                                                  0              \n1                                                  0              \n2                                                  0              \n3                                                  0              \n4                                                  0              \n5                                                  0              \n6                                                  0              \n7                                                  0              \n8                                                  0              \n9                                                  0              \n\n   APICall::Landroid/widget/TableRow.getLeft()  \\\n0                                            0   \n1                                            0   \n2                                            0   \n3                                            0   \n4                                            0   \n5                                            0   \n6                                            0   \n7                                            0   \n8                                            0   \n9                                            0   \n\n   APICall::Landroid/widget/HorizontalScrollView.onKeyUp()  \\\n0                                                  0         \n1                                                  0         \n2                                                  0         \n3                                                  0         \n4                                                  0         \n5                                                  0         \n6                                                  0         \n7                                                  0         \n8                                                  0         \n9                                                  0         \n\n   APICall::Landroid/widget/RatingBar.isFocusable()  \\\n0                                                 0   \n1                                                 0   \n2                                                 0   \n3                                                 0   \n4                                                 0   \n5                                                 0   \n6                                                 0   \n7                                                 0   \n8                                                 0   \n9                                                 0   \n\n   APICall::Landroid/preference/ListPreference.getView()  \\\n0                                                  0       \n1                                                  0       \n2                                                  0       \n3                                                  0       \n4                                                  0       \n5                                                  0       \n6                                                  0       \n7                                                  0       \n8                                                  0       \n9                                                  0       \n\n   APICall::Landroid/widget/LinearLayout.computeHorizontalScrollExtent()  \n0                                                  0                      \n1                                                  0                      \n2                                                  0                      \n3                                                  0                      \n4                                                  0                      \n5                                                  0                      \n6                                                  0                      \n7                                                  0                      \n8                                                  0                      \n9                                                  0                      \n\n[10 rows x 24839 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>API_MIN</th>\n      <th>API</th>\n      <th>Permission::WAKE_LOCK</th>\n      <th>Permission::WRITE_EXTERNAL_STORAGE</th>\n      <th>Permission::ACCESS_NETWORK_STATE</th>\n      <th>Permission::WRITE_SETTINGS</th>\n      <th>Permission::INTERNET</th>\n      <th>Intent::AUDIO_BECOMING_NOISY</th>\n      <th>APICall::Landroid/content/Intent.toUri()</th>\n      <th>APICall::Landroid/view/View.setTag()</th>\n      <th>...</th>\n      <th>APICall::Landroid/widget/ListView.setScaleX()</th>\n      <th>APICall::Landroid/widget/ListView.getScaleX()</th>\n      <th>APICall::Landroid/widget/ScrollView.setOnApplyWindowInsetsListener()</th>\n      <th>APICall::Landroid/widget/ExpandableListView.setNextFocusUpId()</th>\n      <th>APICall::Landroid/widget/ScrollView.setSoundEffectsEnabled()</th>\n      <th>APICall::Landroid/widget/TableRow.getLeft()</th>\n      <th>APICall::Landroid/widget/HorizontalScrollView.onKeyUp()</th>\n      <th>APICall::Landroid/widget/RatingBar.isFocusable()</th>\n      <th>APICall::Landroid/preference/ListPreference.getView()</th>\n      <th>APICall::Landroid/widget/LinearLayout.computeHorizontalScrollExtent()</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>26</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16</td>\n      <td>26</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17</td>\n      <td>25</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16</td>\n      <td>27</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>21</td>\n      <td>29</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21</td>\n      <td>29</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>15</td>\n      <td>23</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>16</td>\n      <td>21</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>21</td>\n      <td>29</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 24839 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1721214755519
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MalwareDetectionNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MalwareDetectionNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 2)  # 2 output classes: benign or malicious\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1721214755734
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    return avg_loss, accuracy"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1721214755948
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_in_chunks(data_asset, chunk_size, model, criterion, optimizer,non_numerical_columns, epochs=1):\n",
        "    \n",
        "    chunks = pd.read_csv(data_asset.path, chunksize=chunk_size,nrows=100000)\n",
        "    \n",
        "  \n",
        "\n",
        "    for chunk_idx, chunk in enumerate(chunks):\n",
        "        print(f\"Processing chunk {chunk_idx+1}\")\n",
        "        \n",
        "        # Train-test split\n",
        "        train_split = int(0.8 * len(chunk))\n",
        "        display(non_numerical_columns)\n",
        "        chunk2=chunk.drop(columns=non_numerical_columns)\n",
        "        x_train = chunk2[:train_split]\n",
        "        x_test = chunk2[train_split:]\n",
        "        #x_train = x_train.drop(columns=non_numerical_columns)\n",
        "        #x_test = x_test.drop(columns=non_numerical_columns)\n",
        "\n",
        "        y_train = x_train['CLASS']\n",
        "        y_test = x_test['CLASS']\n",
        "        #all_test.append(x_test)\n",
        "        #all_test = pd.concat([all_test, x_test], ignore_index=True)\n",
        "\n",
        "        x_train = x_train.drop(columns='CLASS')\n",
        "        x_test = x_test.drop(columns='CLASS')\n",
        "\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        x_train = scaler.fit_transform(x_train)\n",
        "        x_test = scaler.transform(x_test)\n",
        "\n",
        "        # Save the scaler\n",
        "        with open('scaler.pkl', 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "            \n",
        "        # Convert to PyTorch tensors\n",
        "        X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "        X_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "        # Create DataLoader\n",
        "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Chunk {chunk_idx+1}, Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}',flush=True)\n",
        "\n",
        "        # Evaluate on test set after each chunk\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
        "        print(f'Chunk {chunk_idx+1}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}',flush=True)\n",
        "\n",
        "        # Save model state after each chunk\n",
        "        model_save_path = f'model_chunk_2_{chunk_idx+1}.pth'\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model state to {model_save_path}\",flush=True)\n",
        "\n",
        "        # Upload the file to the default datastore\n",
        "        #run = Run.get_context()  # Get the current run context\n",
        "        run.upload_file(name=f\"model_outputs/{model_save_path}\", path_or_stream=model_save_path)\n",
        "        print(f\"Uploaded model state to the default datastore at model_outputs/{model_save_path}\")\n",
        "\n",
        "\n",
        "\n",
        "        # Explicitly delete the chunk and call garbage collection\n",
        "        del chunk, x_train, x_test, y_train, y_test\n",
        "        del X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor\n",
        "        del train_dataset, test_dataset, train_loader, test_loader\n",
        "        gc.collect()\n",
        "\n",
        "        # Optionally: Load model state before processing the next chunk (if needed)\n",
        "        # model.load_state_dict(torch.load(model_save_path))\n"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1721216858321
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chunk_idx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_chunk_2_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chunk_idx' is not defined"
          ]
        }
      ],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721216836294
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate fn"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run.experiment.workspace"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "Workspace.create(name='stefan', subscription_id='439ab49f-8192-408d-b924-846fbe36dfc7', resource_group='stefanshkenderov')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1721214756370
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_path\u001b[49m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721214756523
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721214756788
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = MalwareDetectionNN(input_size)\n",
        "\n",
        "#model = Model(ws, name='MH100K_PartLast_model', version=1)\n",
        "#model_path = model.download(exist_ok=True)\n",
        "\n",
        "# Load your model class definition\n",
        "loaded_model = MalwareDetectionNN(input_size)  # Replace with your model class\n",
        "#loaded_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1721215042657
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the chunk size\n",
        "chunk_size = 5000 #incr and experiment\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(loaded_model.parameters(), lr=0.001)"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721215044763
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721214756835
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_in_chunks(data_asset, chunk_size, loaded_model, criterion, optimizer,non_numerical_columns,10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Processing chunk 1\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 1, Epoch 1, Loss: 0.025994155964115636\nChunk 1, Epoch 2, Loss: 0.032978072073077784\nChunk 1, Epoch 3, Loss: 0.023235523393261247\nChunk 1, Epoch 4, Loss: 0.029372438060527202\nChunk 1, Epoch 5, Loss: 0.030035553243011236\nChunk 1, Epoch 6, Loss: 0.0262973186573945\nChunk 1, Epoch 7, Loss: 0.016291915625333785\nChunk 1, Epoch 8, Loss: 0.019696548927313415\nChunk 1, Epoch 9, Loss: 0.016265867715672358\nChunk 1, Epoch 10, Loss: 0.018911026933186803\nChunk 1, Test Loss: 0.0241, Test Accuracy: 0.9920\nSaved model state to model_chunk_2_1.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_1.pth\nProcessing chunk 2\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 2, Epoch 1, Loss: 0.020530974418152225\nChunk 2, Epoch 2, Loss: 0.0018197962351504575\nChunk 2, Epoch 3, Loss: 0.0012181349907374184\nChunk 2, Epoch 4, Loss: 0.0011798542228274797\nChunk 2, Epoch 5, Loss: 0.001163746448863579\nChunk 2, Epoch 6, Loss: 0.0011505964817834168\nChunk 2, Epoch 7, Loss: 0.0011418970125634188\nChunk 2, Epoch 8, Loss: 0.0011237351611893\nChunk 2, Epoch 9, Loss: 0.0005685558645931295\nChunk 2, Epoch 10, Loss: 0.0005658922440028213\nChunk 2, Test Loss: 0.0023, Test Accuracy: 0.9990\nSaved model state to model_chunk_2_2.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_2.pth\nProcessing chunk 3\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 3, Epoch 1, Loss: 0.0009317566948766682\nChunk 3, Epoch 2, Loss: 0.0005557449674110621\nChunk 3, Epoch 3, Loss: 0.0005540076461321935\nChunk 3, Epoch 4, Loss: 0.0005523894463601362\nChunk 3, Epoch 5, Loss: 0.0005507573941940863\nChunk 3, Epoch 6, Loss: 0.0005491272110099957\nChunk 3, Epoch 7, Loss: 0.00054745181948382\nChunk 3, Epoch 8, Loss: 0.0005457598819257025\nChunk 3, Epoch 9, Loss: 0.0005440029204239547\nChunk 3, Epoch 10, Loss: 0.0005421434229124227\nChunk 3, Test Loss: 3.8066, Test Accuracy: 0.8020\nSaved model state to model_chunk_2_3.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_3.pth\nProcessing chunk 4\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 4, Epoch 1, Loss: 0.8793855150938034\nChunk 4, Epoch 2, Loss: 0.245396602332592\nChunk 4, Epoch 3, Loss: 0.1906348138153553\nChunk 4, Epoch 4, Loss: 0.1625749854147434\nChunk 4, Epoch 5, Loss: 0.14705582830309868\nChunk 4, Epoch 6, Loss: 0.14184008480608462\nChunk 4, Epoch 7, Loss: 0.13580448581278323\nChunk 4, Epoch 8, Loss: 0.12663543316721917\nChunk 4, Epoch 9, Loss: 0.11905142571032047\nChunk 4, Epoch 10, Loss: 0.11344494067877531\nChunk 4, Test Loss: 0.2038, Test Accuracy: 0.9160\nSaved model state to model_chunk_2_4.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_4.pth\nProcessing chunk 5\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 5, Epoch 1, Loss: 0.09096655834533027\nChunk 5, Epoch 2, Loss: 0.00305239146799817\nChunk 5, Epoch 3, Loss: 0.0018892065569336937\nChunk 5, Epoch 4, Loss: 0.0018713045791447112\nChunk 5, Epoch 5, Loss: 0.0018593452932023186\nChunk 5, Epoch 6, Loss: 0.0018451429184335754\nChunk 5, Epoch 7, Loss: 0.0008382941348412451\nChunk 5, Epoch 8, Loss: 8.117504760832617e-06\nChunk 5, Epoch 9, Loss: 7.5970095001469674e-06\nChunk 5, Epoch 10, Loss: 7.119340259476914e-06\nChunk 5, Test Loss: 0.0007, Test Accuracy: 1.0000\nSaved model state to model_chunk_2_5.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_5.pth\nProcessing chunk 6\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 6, Epoch 1, Loss: 0.00036861422226211894\nChunk 6, Epoch 2, Loss: 1.4376930815938493e-05\nChunk 6, Epoch 3, Loss: 1.0960766569187542e-05\nChunk 6, Epoch 4, Loss: 9.307799605345225e-06\nChunk 6, Epoch 5, Loss: 8.13554639216818e-06\nChunk 6, Epoch 6, Loss: 7.1894198174558e-06\nChunk 6, Epoch 7, Loss: 6.419186943256605e-06\nChunk 6, Epoch 8, Loss: 5.756506682560669e-06\nChunk 6, Epoch 9, Loss: 5.121298478997005e-06\nChunk 6, Epoch 10, Loss: 4.642933508655744e-06\nChunk 6, Test Loss: 0.0010, Test Accuracy: 0.9990\nSaved model state to model_chunk_2_6.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_6.pth\nProcessing chunk 7\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 7, Epoch 1, Loss: 0.00014402797852936723\nChunk 7, Epoch 2, Loss: 1.3913820797849752e-05\nChunk 7, Epoch 3, Loss: 9.944561493005821e-06\nChunk 7, Epoch 4, Loss: 7.93648187233309e-06\nChunk 7, Epoch 5, Loss: 6.448678672267505e-06\nChunk 7, Epoch 6, Loss: 5.45766984612861e-06\nChunk 7, Epoch 7, Loss: 4.696023547039019e-06\nChunk 7, Epoch 8, Loss: 4.0959290170476945e-06\nChunk 7, Epoch 9, Loss: 3.6133750262088426e-06\nChunk 7, Epoch 10, Loss: 3.1971509515997808e-06\nChunk 7, Test Loss: 0.0000, Test Accuracy: 1.0000\nSaved model state to model_chunk_2_7.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_7.pth\nProcessing chunk 8\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 8, Epoch 1, Loss: 4.3070044565032714e-05\nChunk 8, Epoch 2, Loss: 2.2040797059830196e-06\nChunk 8, Epoch 3, Loss: 1.8098672879816037e-06\nChunk 8, Epoch 4, Loss: 1.5175097354855893e-06\nChunk 8, Epoch 5, Loss: 1.3154871696556824e-06\nChunk 8, Epoch 6, Loss: 1.163200826592714e-06\nChunk 8, Epoch 7, Loss: 1.0421279696029018e-06\nChunk 8, Epoch 8, Loss: 9.421425115849758e-07\nChunk 8, Epoch 9, Loss: 8.55889467956672e-07\nChunk 8, Epoch 10, Loss: 7.817590303051958e-07\nChunk 8, Test Loss: 0.0000, Test Accuracy: 1.0000\nSaved model state to model_chunk_2_8.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_8.pth\nProcessing chunk 9\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 9, Epoch 1, Loss: 0.00011916580631672602\nChunk 9, Epoch 2, Loss: 2.3811251843852688e-07\nChunk 9, Epoch 3, Loss: 2.1939772746115693e-07\nChunk 9, Epoch 4, Loss: 2.0554046751719567e-07\nChunk 9, Epoch 5, Loss: 1.9353081874307065e-07\nChunk 9, Epoch 6, Loss: 1.832197568898408e-07\nChunk 9, Epoch 7, Loss: 1.7377291156250862e-07\nChunk 9, Epoch 8, Loss: 1.6513064175960323e-07\nChunk 9, Epoch 9, Loss: 1.5738239505402874e-07\nChunk 9, Epoch 10, Loss: 1.5005133890610977e-07\nChunk 9, Test Loss: 0.0000, Test Accuracy: 1.0000\nSaved model state to model_chunk_2_9.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_9.pth\nProcessing chunk 10\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 10, Epoch 1, Loss: 4.1932503390054165e-06\nChunk 10, Epoch 2, Loss: 5.583515934937111e-07\nChunk 10, Epoch 3, Loss: 4.564176995174307e-07\nChunk 10, Epoch 4, Loss: 3.8783571078049307e-07\nChunk 10, Epoch 5, Loss: 3.3828698972371287e-07\nChunk 10, Epoch 6, Loss: 2.9940277692830365e-07\nChunk 10, Epoch 7, Loss: 2.6793670478397755e-07\nChunk 10, Epoch 8, Loss: 2.416846131918504e-07\nChunk 10, Epoch 9, Loss: 2.2028881383384658e-07\nChunk 10, Epoch 10, Loss: 2.0005529197675286e-07\nChunk 10, Test Loss: 0.0001, Test Accuracy: 1.0000\nSaved model state to model_chunk_2_10.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_10.pth\nProcessing chunk 11\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 11, Epoch 1, Loss: 3.1357533286815454e-06\nChunk 11, Epoch 2, Loss: 3.625123951156439e-07\nChunk 11, Epoch 3, Loss: 2.98865787412339e-07\nChunk 11, Epoch 4, Loss: 2.5601603763192317e-07\nChunk 11, Epoch 5, Loss: 2.2514437013221312e-07\nChunk 11, Epoch 6, Loss: 1.9617948303363163e-07\nChunk 11, Epoch 7, Loss: 1.7460451503303887e-07\nChunk 11, Epoch 8, Loss: 1.5678413819131264e-07\nChunk 11, Epoch 9, Loss: 1.4146681918880688e-07\nChunk 11, Epoch 10, Loss: 1.2859306296064688e-07\nChunk 11, Test Loss: 0.0000, Test Accuracy: 1.0000\nSaved model state to model_chunk_2_11.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_11.pth\nProcessing chunk 12\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 12, Epoch 1, Loss: 4.090722522837709e-06\nChunk 12, Epoch 2, Loss: 1.0570570529644386e-07\nChunk 12, Epoch 3, Loss: 8.997112924369332e-08\nChunk 12, Epoch 4, Loss: 8.022624688308611e-08\nChunk 12, Epoch 5, Loss: 7.256737963423632e-08\nChunk 12, Epoch 6, Loss: 6.610051059396937e-08\nChunk 12, Epoch 7, Loss: 6.052766272368615e-08\nChunk 12, Epoch 8, Loss: 5.569982577569022e-08\nChunk 12, Epoch 9, Loss: 5.119979392098628e-08\nChunk 12, Epoch 10, Loss: 4.744479639562371e-08\nChunk 12, Test Loss: 0.0000, Test Accuracy: 1.0000\nSaved model state to model_chunk_2_12.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_12.pth\nProcessing chunk 13\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 13, Epoch 1, Loss: 6.542992495486288e-07\nChunk 13, Epoch 2, Loss: 2.8103414155822292e-08\nChunk 13, Epoch 3, Loss: 2.0652922726682732e-08\nChunk 13, Epoch 4, Loss: 1.7523709313849168e-08\nChunk 13, Epoch 5, Loss: 1.5407757807395227e-08\nChunk 13, Epoch 6, Loss: 1.3828244469493711e-08\nChunk 13, Epoch 7, Loss: 1.2487146452855314e-08\nChunk 13, Epoch 8, Loss: 1.1414267641285391e-08\nChunk 13, Epoch 9, Loss: 1.0460596953976164e-08\nChunk 13, Epoch 10, Loss: 9.685739547293793e-09\nChunk 13, Test Loss: 0.0000, Test Accuracy: 1.0000\nSaved model state to model_chunk_2_13.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_13.pth\nProcessing chunk 14\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 14, Epoch 1, Loss: 1.1154100626953323e-07\nChunk 14, Epoch 2, Loss: 2.926539618464119e-08\nChunk 14, Epoch 3, Loss: 2.253031435195396e-08\nChunk 14, Epoch 4, Loss: 1.8596503236878446e-08\nChunk 14, Epoch 5, Loss: 1.594414525918353e-08\nChunk 14, Epoch 6, Loss: 1.4036823863250447e-08\nChunk 14, Epoch 7, Loss: 1.2367910988686504e-08\nChunk 14, Epoch 8, Loss: 1.1056621710636705e-08\nChunk 14, Epoch 9, Loss: 1.0073153726253281e-08\nChunk 14, Epoch 10, Loss: 9.149288079157714e-09\nChunk 14, Test Loss: 0.0000, Test Accuracy: 1.0000\nSaved model state to model_chunk_2_14.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_14.pth\nProcessing chunk 15\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 15, Epoch 1, Loss: 0.7815053734183312\nChunk 15, Epoch 2, Loss: 0.3067527239322662\nChunk 15, Epoch 3, Loss: 0.26665974551439287\nChunk 15, Epoch 4, Loss: 0.24849051076173784\nChunk 15, Epoch 5, Loss: 0.23793535518646242\nChunk 15, Epoch 6, Loss: 0.23223498374223708\nChunk 15, Epoch 7, Loss: 0.21351465731859207\nChunk 15, Epoch 8, Loss: 0.2051488822698593\nChunk 15, Epoch 9, Loss: 0.21054000821709634\nChunk 15, Epoch 10, Loss: 0.1968933511376381\nChunk 15, Test Loss: 0.5454, Test Accuracy: 0.8980\nSaved model state to model_chunk_2_15.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_15.pth\nProcessing chunk 16\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 16, Epoch 1, Loss: 0.2851715098619461\nChunk 16, Epoch 2, Loss: 0.18312417221069335\nChunk 16, Epoch 3, Loss: 0.15347559052705764\nChunk 16, Epoch 4, Loss: 0.13075471675395967\nChunk 16, Epoch 5, Loss: 0.10520477679744362\nChunk 16, Epoch 6, Loss: 0.09072236268222332\nChunk 16, Epoch 7, Loss: 0.0834415926039219\nChunk 16, Epoch 8, Loss: 0.07703522159717976\nChunk 16, Epoch 9, Loss: 0.07878787621110678\nChunk 16, Epoch 10, Loss: 0.07571898213215172\nChunk 16, Test Loss: 0.3154, Test Accuracy: 0.9430\nSaved model state to model_chunk_2_16.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_16.pth\nProcessing chunk 17\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 17, Epoch 1, Loss: 0.2550895735323429\nChunk 17, Epoch 2, Loss: 0.14362054784595965\nChunk 17, Epoch 3, Loss: 0.11076737549901008\nChunk 17, Epoch 4, Loss: 0.09918920667469501\nChunk 17, Epoch 5, Loss: 0.0983770554214716\nChunk 17, Epoch 6, Loss: 0.0736574835702777\nChunk 17, Epoch 7, Loss: 0.07743475981801748\nChunk 17, Epoch 8, Loss: 0.06443282458931207\nChunk 17, Epoch 9, Loss: 0.07299622130580247\nChunk 17, Epoch 10, Loss: 0.05779522995278239\nChunk 17, Test Loss: 0.2956, Test Accuracy: 0.9370\nSaved model state to model_chunk_2_17.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_17.pth\nProcessing chunk 18\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 18, Epoch 1, Loss: 0.23456763884425164\nChunk 18, Epoch 2, Loss: 0.12277112905681133\nChunk 18, Epoch 3, Loss: 0.09228650248795747\nChunk 18, Epoch 4, Loss: 0.08549544218927622\nChunk 18, Epoch 5, Loss: 0.06599832506664097\nChunk 18, Epoch 6, Loss: 0.07393617352843285\nChunk 18, Epoch 7, Loss: 0.0770658339355141\nChunk 18, Epoch 8, Loss: 0.06483590650185943\nChunk 18, Epoch 9, Loss: 0.054882432612590494\nChunk 18, Epoch 10, Loss: 0.0956468245111173\nChunk 18, Test Loss: 0.3268, Test Accuracy: 0.9420\nSaved model state to model_chunk_2_18.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_18.pth\nProcessing chunk 19\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 19, Epoch 1, Loss: 0.18102875897660853\nChunk 19, Epoch 2, Loss: 0.09720077929645776\nChunk 19, Epoch 3, Loss: 0.08192997851222754\nChunk 19, Epoch 4, Loss: 0.06579663744196296\nChunk 19, Epoch 5, Loss: 0.05743206928763538\nChunk 19, Epoch 6, Loss: 0.056573639383539556\nChunk 19, Epoch 7, Loss: 0.04324289306253195\nChunk 19, Epoch 8, Loss: 0.05129910661606118\nChunk 19, Epoch 9, Loss: 0.055747964109759775\nChunk 19, Epoch 10, Loss: 0.045064351712353526\nChunk 19, Test Loss: 0.4786, Test Accuracy: 0.9240\nSaved model state to model_chunk_2_19.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_19.pth\nProcessing chunk 20\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "['SHA256', 'NOME', 'PACOTE']"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Chunk 20, Epoch 1, Loss: 0.1645863873362541\nChunk 20, Epoch 2, Loss: 0.08047849850542843\nChunk 20, Epoch 3, Loss: 0.05275656422553584\nChunk 20, Epoch 4, Loss: 0.04683008658885956\nChunk 20, Epoch 5, Loss: 0.05112621170282364\nChunk 20, Epoch 6, Loss: 0.041905271855182946\nChunk 20, Epoch 7, Loss: 0.03733301687776111\nChunk 20, Epoch 8, Loss: 0.05673723075632006\nChunk 20, Epoch 9, Loss: 0.07133313863258808\nChunk 20, Epoch 10, Loss: 0.047762916221283376\nChunk 20, Test Loss: 0.2204, Test Accuracy: 0.9470\nSaved model state to model_chunk_2_20.pth\nUploaded model state to the default datastore at model_outputs/model_chunk_2_20.pth\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1721218152252
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path=\"MH100k_1_out.pth\""
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721216458733
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'MH100k_2_out.pth')"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721218189684
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the file is created\n",
        "if os.path.exists(model_save_path):\n",
        "    print(f\"File {model_save_path} exists. Proceeding with upload.\")\n",
        "    # Upload the file\n",
        "    run.upload_file(name=f\"model_outputs/{model_save_path}\", path_or_stream=model_save_path)\n",
        "    print(\"File uploaded successfully.\")\n",
        "else:\n",
        "    print(f\"File {model_save_path} does not exist. Skipping upload.\")\n",
        "    #run.log(\"warning\", f\"File {model_save_path} was not found. Skipping upload.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "File MH100k_1_out.pth does not exist. Skipping upload.\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721216460455
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_save_path = f'mh100kFinal.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Saved model state to {model_save_path}\",flush=True)\n",
        "\n",
        "# Upload the file to the default datastore\n",
        "#run = Run.get_context()  # Get the current run context\n",
        "run.upload_file(name=f\"model_outputs/{model_save_path}\", path_or_stream=model_save_path)\n",
        "print(f\"Uploaded model state to the default datastore at model_outputs/{model_save_path}\")"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Upload the file to the default datastore\n",
        "model_save_path=\"MH100k_final_attempt2.pth\"\n",
        "#run = Run.get_context()  # Get the current run context\n",
        "run.upload_file(name=f\"model_outputs/{model_save_path}\", path_or_stream=model_save_path)\n",
        "print(f\"Uploaded model state to the default datastore at {model_save_path}\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719991048631
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1719991048642
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the model as a new version\n",
        "ws2 = run.experiment.workspace  # Get the workspace from the current run context\n",
        "registered_model = Model.register(workspace=ws2,\n",
        "                                    model_path=model_save_path,  # this is the local path\n",
        "                                    model_name='MH100K_PartLast_model',  # this is the name the model is registered as\n",
        "                                    \n",
        "                                    description=f'Model trained with chunks')\n",
        "print(f\"Registered model as new version\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Registering model MH100K_PartLast_model\nRegistered model as new version\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721218324148
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert to oonx and tflite\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MalwareDetectionNN(input_size)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721203169908
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('mh100kFinal.pth'))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "Ran out of input",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmh100kFinal.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/serialization.py:920\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 920\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721203195547
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, input_size)\n",
        "torch.onnx.export(model, dummy_input, \"MH100k.onnx\", \n",
        "                  input_names=['input'], output_names=['output'])"
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721218391210
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnx"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting onnx\n  Downloading onnx-1.16.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting protobuf>=3.20.2\n  Downloading protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages (from onnx) (1.23.5)\nInstalling collected packages: protobuf, onnx\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.6\n    Uninstalling protobuf-3.19.6:\n      Successfully uninstalled protobuf-3.19.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.11.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.27.2 which is incompatible.\ntensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 5.27.2 which is incompatible.\nmlflow-skinny 2.4.1 requires protobuf<5,>=3.12.0, but you have protobuf 5.27.2 which is incompatible.\ngoogleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\ngoogle-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\nazureml-mlflow 1.51.0 requires azure-storage-blob<=12.13.0,>=12.5.0, but you have azure-storage-blob 12.20.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed onnx-1.16.1 protobuf-5.27.2\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721218534733
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install protobuf==3.20.3"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.27.2\n    Uninstalling protobuf-5.27.2:\n      Successfully uninstalled protobuf-5.27.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.11.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\nazureml-mlflow 1.51.0 requires azure-storage-blob<=12.13.0,>=12.5.0, but you have azure-storage-blob 12.20.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721218667040
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sng4onnx"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting sng4onnx\n  Downloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\nInstalling collected packages: sng4onnx\nSuccessfully installed sng4onnx-1.0.4\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721218850020
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnx2tf import convert\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721218867134
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Path to your ONNX model\n",
        "onnx_model_path = \"MH100k.onnx\"\n",
        "\n",
        "# Output path for the converted TensorFlow model\n",
        "output_path = \"path_to_save_your_model2\"\n",
        "\n",
        "# Convert the ONNX model to TensorFlow\n",
        "convert(\n",
        "    input_onnx_file_path=onnx_model_path,\n",
        "    output_folder_path=output_path\n",
        ")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\u001b[07mModel optimizing started\u001b[0m ============================================================\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/onnx2tf/onnx2tf.py\", line 614, in convert\n    result = subprocess.check_output(\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/subprocess.py\", line 411, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/subprocess.py\", line 489, in run\n    with Popen(*popenargs, **kwargs) as process:\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/subprocess.py\", line 854, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/subprocess.py\", line 1702, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nFileNotFoundError: [Errno 2] No such file or directory: 'onnxsim'\n\n\u001b[33mWARNING:\u001b[0m Failed to optimize the onnx file.\n\n\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n\n\u001b[07mModel loaded\u001b[0m ========================================================================\n\n\u001b[07mModel conversion started\u001b[0m ============================================================\n\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input \u001b[32mshape\u001b[0m: [1, 24838] \u001b[32mdtype\u001b[0m: float32\n\u001b[33mWARNING:\u001b[0m The optimization process for shape estimation is skipped because it contains OPs that cannot be inferred by the standard onnxruntime.\n\u001b[33mWARNING:\u001b[0m [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: /onnxruntime_src/onnxruntime/core/graph/model.cc:149 onnxruntime::Model::Model(onnx::ModelProto&&, const PathString&, const IOnnxRuntimeOpSchemaRegistryList*, const onnxruntime::logging::Logger&, const onnxruntime::ModelOptions&) Unsupported model IR version: 10, max supported IR version: 9\n\n\n\u001b[32mINFO:\u001b[0m \u001b[32m2 / 6\u001b[0m\n\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: Gemm_0\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input \u001b[36mshape\u001b[0m: [1, 24838] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: fc1.weight \u001b[36mshape\u001b[0m: [64, 24838] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: fc1.bias \u001b[36mshape\u001b[0m: [64] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: onnx____Relu_7 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 24838) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (24838, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (64,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\n\u001b[32mINFO:\u001b[0m \u001b[32m3 / 6\u001b[0m\n\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Relu\u001b[35m onnx_op_name\u001b[0m: Relu_1\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: onnx____Relu_7 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: onnx____Gemm_8 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: relu\n\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu/Relu:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\n\u001b[32mINFO:\u001b[0m \u001b[32m4 / 6\u001b[0m\n\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: Gemm_2\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: onnx____Gemm_8 \u001b[36mshape\u001b[0m: [1, 64] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: fc2.weight \u001b[36mshape\u001b[0m: [32, 64] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: fc2.bias \u001b[36mshape\u001b[0m: [32] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: onnx____Relu_9 \u001b[36mshape\u001b[0m: [1, 32] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 64) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (64, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (32,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_1/AddV2:0 \u001b[34mshape\u001b[0m: (1, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\n\u001b[32mINFO:\u001b[0m \u001b[32m5 / 6\u001b[0m\n\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Relu\u001b[35m onnx_op_name\u001b[0m: Relu_3\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: onnx____Relu_9 \u001b[36mshape\u001b[0m: [1, 32] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: onnx____Gemm_10 \u001b[36mshape\u001b[0m: [1, 32] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: relu\n\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_1/AddV2:0 \u001b[34mshape\u001b[0m: (1, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu_1/Relu:0 \u001b[34mshape\u001b[0m: (1, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\n\u001b[32mINFO:\u001b[0m \u001b[32m6 / 6\u001b[0m\n\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: Gemm_4\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: onnx____Gemm_10 \u001b[36mshape\u001b[0m: [1, 32] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: fc3.weight \u001b[36mshape\u001b[0m: [2, 32] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: fc3.bias \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: output \u001b[36mshape\u001b[0m: [1, 2] \u001b[36mdtype\u001b[0m: float32\n\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (32, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_2/AddV2:0 \u001b[34mshape\u001b[0m: (1, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n\n\u001b[07msaved_model output started\u001b[0m ==========================================================\n\u001b[32msaved_model output complete!\u001b[0m\n\u001b[32mFloat32 tflite output complete!\u001b[0m\n\u001b[32mFloat16 tflite output complete!\u001b[0m\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "<keras.engine.functional.Functional at 0x7efdef8f1eb0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721218884324
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}